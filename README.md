# Multi-Layer-Perceptron
<h1>GUI</h1>
<h5>User Input:</h5>
<p>Enter number of hidden layers</p>
<p>Enter number of neurons in each hidden layer</p>
<p>Enter learning rate (eta)</p>
<p>Enter number of epochs (m)</p>
<p>Add bias or not (Checkbox)</p>
<p>Choose to use Sigmoid or Hyperbolic Tangent sigmoid as the activation function</p>

<br>
<h1>Initialization:</h1>
<p>Number of features = 5</p>
<p>Number of classes = 3.</p>
<p>Weights + Bias = small random numbers</p>
<br>
<h1>Classification:</h1>
<p>Sample (single sample to be classified)</p>
<br>
<h1>Description:</h1>
<p>Implement the Back-Propagation learning algorithm on a multi-layer neural networks, which can be able to classify a stream of input data to one of a set of predefined classes.</p>
<p>Use the penguins or Iris data in both your training and testing processes. (Each class has 50 samples: train NN with the first 30 non-repeated samples, and test it with the remaining 20 samples)</p>
<br>
<h5>After training:</h5>
<p>Test the classifier with the remaining 20 samples of each selected classes and find confusion matrix and compute overall accuracy.</p>
<br>
<h1>Notes:</h1>
<p>You should not drop any row from the dataset.</p>
<p>Using scikit-learn metrics library or any similar built-in function for the confusion matrix is not allowed.</p>
